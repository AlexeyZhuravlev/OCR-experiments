SharedParams:
  - &vocab_path C:\Users\User\Documents\PhD-study\OCR-TextRecognition-Data\Compiled\Sample\Vocab.txt
  - &data_root C:\Users\User\Documents\PhD-study\OCR-TextRecognition-Data\Compiled 
  - &log_dir C:\Users\User\Documents\PhD-study\Logs
  - &image_height 64

data_registry:
  rootdir: *data_root

model_params:
  model: MultiHeadOcrModel
  vocab_path: *vocab_path
  input_height: *image_height

  feature_extractor_name: simple
  feature_extractor_params:
    output_channels: 256
  heads_params:
    ctc_head:
      type: lstm_ctc
      specific_params:
        lstm_args:
          hidden_size: 100
          bidirectional: True

args:  # REQUIRED KEYWORD, various arguments for Catalyst
  logdir: *log_dir
  #baselogdir: /path/to/baselogdir  # KEYWORD optional argument -- path for the root with logs (if it is specified but not logdir, then the logdir will be generated as `{baselogdir}/{ConfigExperiment._get_logdir(config)}`)
  expdir: "src"  # REQUIRED KEYWORD, the path to your experiment, with the file `__init__` in which you import Experiment, Runner, and optionally all additional entities are registered: model, callback, criterion, etc
  seed: 42  # KEYWORD, training seed for PyTorch, Numpy, Python and Tensorflow (Default is 42)
  deterministic: True  # KEYWORD, whether to use deterministic CuDNN (Default is True)
  benchmark: True  # KEYWORD, whether to use CuDNN benchmark
  verbose: False  # KEYWORD, whether to display learning information on the console (Default is False)
  check: False  # KEYWORD, if True, then Catalyst does only 3 epochs (to check the performance of the pipeline, by default False)


runner_params:  # OPTIONAL KEYWORD, params for Runner's init
  input_key: image
  input_target_key: string
  output_key: output


stages:  # REQUIRED KEYWORD, dictionary of all stages of Catalyst, for training and/or infer. Contain keywords with parameters that apply to all the stages, as well as the names of the stages
  data_params:  # KEYWORD, parameters passed to `ConfigExperiment.get_datasets(...)` (for all the stages)
    dataset_names:
      train: CAR_TRAIN
      valid: CAR_TEST
    image_resize_params:
      # All images are firstly resized to have fixed height keeping aspect ratio
      height: *image_height 
      # Allowed images width range. Smaller images are padded to max_width on the right side
      min_width: 256
      max_width: 256
    batch_size: 8  # KEYWORD, batch size for all the stages
    num_workers: 0  # KEYWORD, Number of parallel processes for DataLoader
    drop_last: False  # KEYWORD, parameter for DataLoader (Default is False)
    per_gpu_scaling: False  # KEYWORD, if True and the working mode are not distributed, it increases the batch size and the number of workers in proportion to the number of GPUs
    loaders_params:  # KEYWORD, parameters for loaders, optional
      valid:  # Overrides the value for valid loader
        batch_size: 32

  state_params:  # REQUIRED KEYWORD, parameters for State (for all stages)
    main_metric: &main_metric accuracy01  # REQUIRED KEYWORD, the name of the metric by which the checkpoints will be taken
    minimize_metric: False  # REQUIRED KEYWORD, flag, should we minimize `main_metric`
    num_epochs: 2  # KEYWORD, The number of epochs in all the stages
    valid_loader: valid  # KEYWORD, name of the loader by which the checkpoints will be taken

  criterion_params:  # REQUIRED KEYWORD, parameters for the loss function
    _key_value: False  # KEYWORD, if True, there may be several loss-functions and then they should be wrapped in key-value

    criterion: BCEWithLogitsLoss  # REQUIRED KEYWORD, name of the loss function
    # At this level, the parameters for __init__ of the loss function can be found, for example
    reduction: sum

  optimizer_params:  # REQUIRED KEYWORD, parameters for the optimizer
    _key_value: False  # KEYWORD, if True, there may be several optimizers and then they should be wrapped in key-value
    layerwise_params:  # KEYWORD, optimizer parameters for different network layers, optional
      conv1.*:  # regexp with layer name
        lr: 0.001
        weight_decay: 0.0003
      encoder.conv.*:
        lr: 0.002
    no_bias_weight_decay: True  # KEYWORD whether to remove weight decay from the all bias parameters of the network (Default is True)
    lr_linear_scaling:  # KEYWORD, parameters for linear lr scaling
      lr: 0.001
      base_batch_size: 64  # KEYWORD, size of the base batch size before scaling

    optimizer: Adam  # REQUIRED KEYWORD, name of the optimizer
    # At this level, the parameters for __init__ of the optimizer can be found, for example
    lr: 0.003
    weight_decay: 0.0001

  scheduler_params:  # REQUIRED KEYWORD, params for lr-scheduler
    _key_value: False  # KEYWORD, if True, there may be several lr-schedulers and then they should be wrapped in key-value

    scheduler: StepLR  # REQUIRED KEYWORD, name of the lr-scheduler
    # At this level, the parameters for __init__ of the lr-scheduler can be found, for example
    step_size: 10
    gamma: 0.3

  train:  # Anything that's not a keyword is considered a name for a stage. For training in Catalyst, at least one stage is required. The name can be anything.
    state_params:  # You can override any parameters for a particular stage, for example
      num_epochs: 3

    callbacks_params:  # REQUIRED KEYWORD, The most important part. It's where all the callbacks are written down for this stage.
    # Callbacks are written through key-value
      loss:
        callback: CriterionCallback  # KEYWORD name of the callback
      optimizer:
        callback: OptimizerCallback
      scheduler:
        callback: SchedulerCallback
        # At this level, the parameters for any callback can be found, for example
        reduced_metric: *main_metric
      saver:
        callback: CheckpointCallback
        save_n_best: 3

  finetune:  # Example of a second training stage, here we can change our parameters
    state_params:  # You can override any parameters for a particular stage, for example
      num_epochs: 1

    optimizer_params:  # Example of an overridden optimizer
      load_from_previous_stage: True  # KEYWORD, a flag that says you need to load statistics from the previous stage
      optimizer: Adam
      lr: 0.001

  # the number of stages can be as many as you like.